# 自监督分类：生成式、判别式

Reference: https://www.zhihu.com/question/374171361/answer/2289248630

- 生成式：期望利用数据表示重构完整数据

- 判别式：期望数据表示包含足够多信息

![](assets/2022-08-01-09-46-04-2022-08-01%2009-45-45%20的屏幕截图.png)

## 自监督评价方法

1. 线性评估：对于下游任务，直接在训练好的encoder后面加一个线性分类器，之后测试模型在下游任务上的表现，因为分类器十分简单，所以整体模型的好坏，主要归因于encoder的性能。

2. 下游任务fine tune：设计一个分类器，之后利用下游任务的有标注数据集对数据进行fine tune。将微调后的模型作为执行下游任务的模型。

## 自监督方式

1. 基于上下文（Context Based）

2. 基于时序（Temporal Based）

3. 基于对比学习（Contrastive Based）

4. 基于蒸馏（distillation loss）

---

## 3D

---

---

### 基于上下文：通常人们可以通过理解一整幅图像的语义，并联系图像中图案的结构来补全一个诠释的图像。

#### 1. MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation

参考 | [https://zhuanlan.zhihu.com/p/360486586](https://zhuanlan.zhihu.com/p/360486586)

论文 | https://arxiv.org/pdf/2103.12605

引用 | CVPR2021

---

### 基于时序

    参考：[https://flashgene.com/archives/106955.html](https://flashgene.com/archives/106955.html)

1. 基于帧之间的相似性：相邻帧特征是相似的，而相隔较远的视频帧是不相似的，通过构建这种相似（position）和不相似（negative）的样本来进行自监督约束。

![](assets/2022-08-01-10-21-04-2022-08-01%2010-20-51%20的屏幕截图.png)

2. 在大量无标注视频中进行无监督追踪，获取大量的物体追中框。同一个物体在不同帧之间的相似， 不同物体的追踪框不同。

![](assets/2022-08-01-10-20-13-2022-08-01%2010-20-03%20的屏幕截图.png)

3. 视频的先后帧顺序：设计一个模型，来判断当前的视频序列是否是正确的顺序。
   
   ![](assets/2022-08-01-10-24-11-2022-08-01%2010-24-01%20的屏幕截图.png)

---

### 基于对比

#### 1. SimIPU: Simple 2D Image and 3D Point Cloud Unsupervised Pre-training for Spatial-Aware Visual Representations

论文 | https://ojs.aaai.org/index.php/AAAI/article/view/20040/19799

输入：2D图像和Lidar数据

![](assets/2022-08-01-14-37-09-2022-08-01%2014-36-54%20的屏幕截图.png)

点云$p^\alpha$， 经过T变换之后为 $P^\beta$ ，对$P^\alpha 和 p^\beta$ 提取特征，通过 二分图匹配， 计算对比损失；然后 2D image提取特征， feature map， 与$P^\alpha$ 做对比损失。

结果

‘K’ and ‘IN’ indicates pre-trained models are trained on KITTI and ImageNet datset. Best is in bold.

‘W’ indicates the  pre-trained models are trained on Waymo datset.

![](assets/2022-08-01-14-39-43-2022-08-01%2014-39-35%20的屏幕截图.png)

![](assets/2022-08-01-14-41-12-2022-08-01%2014-39-47%20的屏幕截图.png)

点云数据对无监督的3D目标检测， 相较于Mocov2有一定提升。

---

## 2D

参考：

- [综述：自监督学习与知识蒸馏的碰撞 - 智源社区](https://hub.baai.ac.cn/view/4380)

- https://www.zhihu.com/question/374171361/answer/2289248630

- https://zhuanlan.zhihu.com/p/334732028

- https://zhuanlan.zhihu.com/p/393008809

### 2D的前置任务（pretext）——提取数据分布潜在规律的关键

| 变形      | 如果能够赋予神经网络这种利用不变性的能力，则可能有效应对对抗性样本的攻击。例如，Dosovitskiy 在于2015年发表的「Exemplar-CNN」中                                                                                         | ![](assets/2022-08-01-18-16-20-2022-08-01%2018-16-09%20的屏幕截图.png) |
| ------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------- |
| 相对位置预测  | 对于每一张输入图像，我们期望网络可以学习到不同图块之间的相对位置关系。Doersch 等人于 2015 年发表的论文「Unsupervised Visual Representation Learning by Context Prediction」将 pretext 任务形式化定义为预测同一张图像中随机两个图块之间的相对位置。 | ![](assets/2022-08-01-18-18-18-2022-08-01%2018-18-11%20的屏幕截图.png) |
| 图像着色    | 图像自动上色也可以作为一种有效的前置任务。在 ECCV 2016 上发表的论文「Colorful Image Colorization」中，作者提出通过量化色彩空间，将着色问题转化而我分类问题，将该图像映射到量化的色彩值输出的分布上。                                                 | ![](assets/2022-08-01-18-18-34-2022-08-01%2018-18-25%20的屏幕截图.png) |
| 生成式图像恢复 | 自编码器是一类在自监督学习中被广泛使用的网络架构，旨在通过重建被扰动的图像提取出关于图像的本质特征。在论文「Context Encoders: Feature Learning by Inpainting」中，作者训练了一个上下文编码器来填补图像中缺失的一块，并使用了一个 L2 重建损失和对抗性损失的组合来训练该模型。      | ![](assets/2022-08-01-18-18-45-2022-08-01%2018-18-36%20的屏幕截图.png) |

### 基于对比

##### 总结

参考 | https://github.com/mli/paper-reading/blob/main/README.md

视频 | https://www.bilibili.com/video/BV19S4y1M7hm/?spm_id_from=333.788&vd_source=99d7235d02230cfcd19f9418d0876854

| 年份   | 名字                                                 | 简介                                   |
| ---- | -------------------------------------------------- | ------------------------------------ |
| 2018 | [InstDisc](https://arxiv.org/pdf/1805.01978.pdf)   | 提出实例判别和memory bank做对比学习              |
| 2018 | [CPC](https://arxiv.org/pdf/1807.03748.pdf)        | 对比预测编码，图像语音文本强化学习全都能做                |
| 2019 | [InvaSpread](https://arxiv.org/pdf/1904.03436.pdf) | 一个编码器的端到端对比学习                        |
| 2019 | [CMC](https://arxiv.org/pdf/1906.05849.pdf)        | 多视角下的对比学习                            |
| 2019 | [MoCov1](https://arxiv.org/pdf/1911.05722.pdf)     | 无监督训练效果也很好                           |
| 2020 | [SimCLRv1](https://arxiv.org/pdf/2002.05709.pdf)   | 简单的对比学习 (数据增强 + MLP head + 大batch训练) |
| 2020 | [MoCov2](https://arxiv.org/pdf/2003.04297.pdf)     | MoCov1 + improvements from SimCLRv1  |
| 2020 | [SimCLRv2](https://arxiv.org/pdf/2006.10029.pdf)   | 大的自监督预训练模型很适合做半监督学习                  |
| 2020 | [BYOL](https://arxiv.org/pdf/2006.07733.pdf)       | 不需要负样本的对比学习                          |
| 2020 | [SWaV](https://arxiv.org/pdf/2006.09882.pdf)       | 聚类对比学习                               |
| 2020 | [SimSiam](https://arxiv.org/pdf/2011.10566.pdf)    | 化繁为简的孪生表征学习                          |
| 2021 | [MoCov3](https://arxiv.org/pdf/2104.02057.pdf)     | 如何更稳定的自监督训练ViT                       |
| 2021 | [DINO](https://arxiv.org/pdf/2104.14294.pdf)       | transformer加自监督在视觉也很香                |

---

#### Unsupervised Object-Level Representation Learning from Scene Images

单位 | 

作者 | 

论文 | NeurIPS 2021 https://arxiv.org/abs/2106.11952

引用 |

---

#### MoCo: Momentum Contrast for Unsupervised Visual Representation Learning

单位 |Facebook AI Research (FAIR)

作者 |Kaiming He Haoqi Fan Yuxin Wu Saining Xie Ross Girshick

论文 | https://arxiv.org/pdf/1911.05722

引用 |CVPR2020

代码 | https://github.com/facebookresearch/moco

##### 主要思想

- Memory Bank容量大， 导致了采样的特征具有不一致性，因为 训练一段时间后， 更新了参数， 导致 encoder结果在 特征空间 不一致

- MoCo采用队列存储和采样Negetive 样本， 队列中存储多个近期用于训练的batch的特征向量。队列存储的是图像特征， 因为队列的FIFO特性， 保证 很长时间前的数据 pop，不会被取到。

##### 模型

<img src="file:///home/yihang/.config/marktext/images/2022-07-27-15-32-02-2022-07-27%2015-31-40%20的屏幕截图.png" title="" alt="" data-align="center">

**对于每个batch x：**

1. 随机增强出 $x^q、x^k$两种view
2. 分别用$f_q、f_k$  对输入进行编码得到归一化的 q 和 k，并去掉 k 的梯度更新
3. 将 q 和 k 一一对应相乘得到正例的cosine（Nx1），再将 q 和队列中存储的K个负样本相乘（NxK），拼接起来的到 Nx(1+K) 大小的矩阵，这时第一个元素就是正例，直接计算交叉熵损失，更新$f_q$的参数
4. 动量更新$f_k$的参数：$f_k=m*f_k+(1-m)*f_q$
5. 将 k 加入队列，把队首的旧编码出队，负例最多时有65536个

##### 数据集

ImageNet-1M (IN-1M)， Instagram-1B (IG-1B):

##### 实验结果

![](assets/2022-08-05-15-19-52-2022-08-05%2015-19-39%20的屏幕截图.png)---

#### SimCLR: A Simple Framework for Contrastive Learning of Visual Representations

作者 |Ting Chen 1 Simon Kornblith 1 Mohammad Norouzi 1 Geoffrey Hinton

论文 | https://arxiv.org/pdf/2002.05709

引用 |ICML2020

        SimCLR是Hinton组的Chen Ting在20年2月提出的工作，直接比MoCo高出了7个点，并直逼监督模型的结果。

<img src="https://pic4.zhimg.com/80/v2-dc3f182be959bfd5757d5f6ffe98410b_720w.jpg" title="" alt="" data-align="center">

##### 对比MoCo

1. 探究了不同的数据增强组合方式，选取了最优的
2. 在encoder之后增加了一个非线性映射 $g(h_i)=W^2 ReLU(W^1 h_i)$。研究发现encoder编码后的$h$会保留和数据增强变换相关的信息，而非线性层的作用就是去掉这些信息，让表示回归数据的本质。注意非线性层只在无监督训练时用，在迁移到其他任务时不使用
3. 计算loss时多加了负例。以前都是拿右侧数据的N-1个作为负例，SimCLR将左侧的N-1个也加入了进来，总计2(N-1)个负例。另外SImCLR不采用memory bank，而是用更大的batch size，最多的时候bsz为8192，有16382个负例

##### 实验结果

![](https://pic4.zhimg.com/80/v2-1a8225d520897fbc359f53fe075ebf67_720w.jpg)

---

#### MoCo v2:  Improved Baselines with Momentum Contrastive Learning

单位 |Facebook AI Research (FAIR)

作者 | Xinlei Chen, Haoqi Fan, Ross Girshick, Kaiming He

论文 | https://arxiv.org/pdf/2003.04297

SimCLR推出后一个月，何凯明和Chen Xinlei同学对MoCo进行了一些小改动：

1. 改进了数据增强方法
2. 训练时在encoder的表示上增加了相同的非线性层
3. 为了对比，学习率采用SimCLR的cosine衰减

##### 模型

<img src="https://pic2.zhimg.com/80/v2-66d1f1e5721901c1c3a24cd34f19dfc1_720w.jpg" title="" alt="" data-align="center">

##### 实验结果

![](https://pic2.zhimg.com/80/v2-a7dbc04be15d66cbcb5fbaf6eccb3d69_720w.jpg)

---

#### SimCLR v2：Big Self-Supervised Models are Strong Semi-Supervised Learners

单位 | Google Research, Brain Team

作者 | Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, Geoffrey Hinton

论文 |  https://arxiv.org/abs/2006.10029

引用 | NIPS2020

代码 | https://github.com/google-research/simclr

##### 对比SimCLR v1改进

在2020年中，Hinton组的Chen Ting同学又提出了SimCLR v2，主要做了以下改动：

1. 采用更深但维度略小的encoder，从 ResNet-50 (4×) 改到了 ResNet-152 (3×+SK)，在1%的监督数据下提升了29个点
2. 采用更深的3层MLP，并在迁移到下游任务时保留第一层（以前是完全舍弃），在1%的监督数据下提升了14个点
3. 参考了MoCo使用memory，但由于batch已经足够大了，只有1%左右的提升

##### 模型迁移

![](assets/2022-08-05-15-30-01-2022-08-05%2015-29-42%20的屏幕截图.png)

左侧训练大模型（任务无关的），然后使用部分 labeled数据进行监督微调，然后通过知识蒸馏的方式迁移到  小一点的 任务相关的模型中。

##### 实验结果

最终模型比之前的SOTA好了不少：

<img src="https://pic2.zhimg.com/80/v2-07718a4d79ffa88c8b12a10844f12349_720w.jpg" title="" alt="" data-align="center">

---

#### MoCo v3：An Empirical Study of Training Self-Supervised Vision Transformers

参考 | [MoCoV3：何恺明团队新作！解决Transformer自监督训练不稳定问题！_夕小瑶的博客-CSDN博客](https://blog.csdn.net/xixiaoyaoww/article/details/115500094)

单位 | Facebook AI Research (FAIR)

作者 | [Xinlei Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+X), [Saining Xie](https://arxiv.org/search/cs?searchtype=author&query=Xie%2C+S), [Kaiming He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+K)

论文 | https://arxiv.org/pdf/2104.02057

代码 | https://github.com/facebookresearch/moco-v3

##### 出发点

    Transformer(ViT) + 自监督

##### 研究方向

针对Transformer在自监督学习框架中存在的训练不稳定问题，提出了一种简单而有效的技巧：Random Patch Projection，它不仅适用于MoCoV3框架，同样适用于其他自监督学习框架(比如SimCLR、BYOL)；与此同时，从不同角度的对比分析得出：Transformer中的位置信息尚未得到充分探索，即Transformer仍有继续改善的空间。研究了影响深度网络训练的几个基本模块：<font color=red>batch size，learning rate以及optimizer</font>。

问题

- **在不同场景下，不稳定性均是影响自监督ViT训练的主要问题**。

- **不稳定的ViT训练可能不会导致灾难性结果(比如发散)；相反，它可以导致精度的轻度退化(约下降1-3%)**。

- **基于梯度变换的经验观察，我们固化ViT中的块投影层，即采用固定随机块投影**。

- **移除ViT中的position embedding仅仅造成了轻微的性能下降**，这意味着：**自监督ViT无需位置信息即可学习很强的特征表达，同时也也暗示位置信息并未得到充分探索**。

##### 模型

![](assets/2022-08-05-18-44-19-2022-08-05%2018-44-00%20的屏幕截图.png)

![](https://pic2.zhimg.com/80/v2-f28f4cab14cb6d63e1e6b9cef7b6e571_720w.jpg)

##### 实验结果

![](https://pic3.zhimg.com/80/v2-3c94f972056252d1f9f57aade8623bb2_720w.jpg)

---

#### SwAV： Unsupervised Learning of Visual Features by Contrasting Cluster Assignments

单位 | 1 Inria∗ 2 Facebook AI Research

作者 | Mathilde Caron1,2 Ishan Misra2 Julien Mairal1  Priya Goyal2 Piotr Bojanowski2 Armand Joulin2

论文 | https://proceedings.neurips.cc/paper/2020/file/70feb62b69f16e0238f741fab228fec2-Paper.pdf

引用 |NIPS2020

        对比学习需要很多负例进行比较，既耗时又耗内存，于是FAIR联合Inria也推出了一个新的方法SwAV。作者提出了一个新的想法：**对各类样本进行聚类，然后去区分每类的类簇**。模型结构如下：

![](https://pic3.zhimg.com/80/v2-5c57e440e11930c0397ddbdb0164c89e_720w.jpg)

        具体的做法是，先用K个点$C=c_1, c_2...c_k$  表示聚类中心（prototypes），给定一个batch的编码 $Z=z_1, z_2, ..., z_B$，将其通过$C$映射到一组新的向量$Q=q_1, q_2, ..., q_B$。这里假设向量都是d维的，那C的维度就是 dxK，Z的维度是 dxB，Q的维度则是KxB，每个元素$q_{kb}$相当于第k个聚类中心与第b个样本的相似度，理想情况下，样本与自己的类簇中心相似度为1，与其他的为0，其实就是一个one-hot label。不过作者发现soft label效果会好一些。这样每个样本又获得了一个新的表示（Codes）。

        有了z和q之后，理论上同一张图片不同view所产生的z和q也可以相互预测，作者便定义了新的loss：$L(z_t, z_s)=l(z_t, q_s)+l(z_s, q_t)$

        其中$l(z_t, q_s)=-\sum\limits_{k}q_s^k\;log\,p_t^k, p_t=\frac{exp(z_t^Tc_k/\tau)}{\sum\limits_{k^{\prime}}exp(z_t^Tc^{k^{\prime}/\tau})}$

        ![](assets/2022-08-05-17-54-41-2022-08-05%2017-54-15%20的屏幕截图.png)

同时SwAV也提出了一种新的数据增强方法，将不同分辨率的view进行mix。最终两种方法的结合带来了4.2个点的提升:

<img src="https://pic3.zhimg.com/80/v2-a6faf2ec006afe9ce5665bbe4c22ce3a_720w.jpg" title="" alt="" data-align="center">

---

#### SEER： Self-supervised Pretraining of Visual Features in the Wild

参考 | https://zhuanlan.zhihu.com/p/361828863

单位 | 1 Facebook AI Research 2 Inria*

作者 | [Priya Goyal](https://arxiv.org/search/cs?searchtype=author&query=Goyal%2C+P), [Mathilde Caron](https://arxiv.org/search/cs?searchtype=author&query=Caron%2C+M), [Benjamin Lefaudeux](https://arxiv.org/search/cs?searchtype=author&query=Lefaudeux%2C+B), [Min Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+M), [Pengchao Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+P), [Vivek Pai](https://arxiv.org/search/cs?searchtype=author&query=Pai%2C+V), [Mannat Singh](https://arxiv.org/search/cs?searchtype=author&query=Singh%2C+M), [Vitaliy Liptchinsky](https://arxiv.org/search/cs?searchtype=author&query=Liptchinsky%2C+V), [Ishan Misra](https://arxiv.org/search/cs?searchtype=author&query=Misra%2C+I), [Armand Joulin](https://arxiv.org/search/cs?searchtype=author&query=Joulin%2C+A), [Piotr Bojanowski](https://arxiv.org/search/cs?searchtype=author&query=Bojanowski%2C+P)

论文 | https://arxiv.org/abs/2103.01988

代码 | [Code found at https://github.com/facebookresearch/vissl](https://www.catalyzex.com/redirect?url=https://github.com/facebookresearch/vissl)

        SEER是SwAV的作者们在21年3月发布的新工作，概括来说就是**用更好的模型、更多的数据。**

        首先是数据，之前的模型都是在一百万左右的ImageNet上训练，而这些数据都是挑选过的，只能代表开放域中的一部分数据。于是作者就想：**在更大的开放域数据上训练是否能提升效果呢？**如果可行的话，那以后就在更更多的数据上pretrain，然后再迁移到下游任务就可以了。于是作者收集了10亿的Instagram图片来做训练。

        然后是模型，作者选用了凯明大神去年推出的RegNet，结合了神经网络搜索NAS的优点，在Imagenet上搜索出特定FLOPs下不错的结构。最后在参数比SimCLRv2少的情况下达到了更好的效果：

<img src="https://pic3.zhimg.com/80/v2-06d4d9db821c0afd734a6b2f69391eee_720w.jpg" title="" alt="" data-align="center">

<img src="https://pic1.zhimg.com/80/v2-82372e17beb0c16c52076b00e0a5c2ec_720w.jpg" title="" alt="" data-align="center">

        虽然证明了在开放域数据上的预训练确实有效果，**但少样本的情况下还是不如直接在ImageNet上训。不过迁移能力确实很好，在Places205数据集上评估，比ImageNet有监督预训练的模型好，说明**无监督预训练让模型学到更多通用知识。

---

#### BYOL：Bootstrap Your Own Latent A New Approach to Self-Supervised Learning

参考 | https://zhuanlan.zhihu.com/p/352364087

单位 | 1DeepMind 2Imperial College

作者 | Jean-Bastien Grill∗,1 Florian Strub∗,1 Florent Altché∗,1 Corentin Tallec∗,1 Pierre H. Richemond∗,1,2  Elena Buchatskaya1 Carl Doersch1 Bernardo Avila Pires1 Zhaohan Daniel Guo1  Mohammad Gheshlaghi Azar1 Bilal Piot1 Koray Kavukcuoglu1 Rémi Munos1 Michal Valko1

论文 | https://arxiv.org/abs/2006.07733

代码 | https://github.com/lucidrains/byol-pytorch

        上文讲的方法来回都逃不过“对比”这个范式，而DeepMind提出的BYOL则给了我们一个不同视角。

        在表示学习中，我们现在采用的框架本质是通过一个view的表示去预测相同图像其他view，能预测对说明抓住了图像的本质特征。但在做这样的预测时会有坍缩（collapse）的风险，意思是全都变成一个表示，那也可以做到预测自己。对比学习为了解决这个问题，将表示预测问题转换为了正负例判别问题，这样就迫使模型的输出是多样的，避免坍缩。

        于是BYOL的作者想：如何不用负例，也能学到好的表示呢？如果共用encoder，用MSE作为损失，缩小相同图像不同view的距离，肯定会坍缩。而作者发现如果把其中一个encoder变成随机初始化的固定下来（stop gradient），就能达到18.8%的准确率。为了得到更好的encoder，作者参考动量的方法对其中一个encoder做了改进：

<img src="https://pic3.zhimg.com/80/v2-f4e062422f439c21aad3fc1480598dbe_720w.jpg" title="" alt="" data-align="center">

        上半部分为online（更新梯度），下半部分为target（不更新梯度）。BYOL的优化目的是用online表示预测target表示，采用MSE作为损失函数。Online梯度回传后，使用滑动平均对targe的encoder和MLP参数进行更新。在预测阶段只使用$f_\theta$。

        虽然BYOL没有显示地使用对比学习loss，但一篇博主在实验中发现BYOL依靠的还是“对比”。他们在复现BYOL的时候直接基于了MoCo的代码，结果发现效果还没有随机的好，原来是因为MLP中没有加BN。如果深究BN的作用，就会发现它重新调整了输出的分布，避免坍缩，同时**BN也在隐式地进行对比，去除batch内样本相同的部分，保留不同的特征**。

同时，在不依赖负样本后，BYOL对于数据增强方法的选择更加鲁棒，下面是它的效果：

<img src="https://pic4.zhimg.com/80/v2-5259ea1d1af06d65b56e6798496b55a7_720w.jpg" title="" alt="" data-align="center">

---

---

#### SimSiam： Exploring Simple Siamese Representation Learning

单位 | Facebook AI Research (FAIR)

作者 | Xinlei Chen Kaiming He

论文 | https://arxiv.org/abs/2011.10566

引用 | CVPR2021

代码 | https://github.com/facebookresearch/simsiam

        延续BYOL的思想，Chen Xinlei与何凯明大佬又对孪生网络进行了研究，发现stop-gradient是避免坍缩的关键，于是提出了SimSiam。

    相较于BYOL， SimSiam不需要进行 student对teacher参数的动量更新，可以理解为student-student ,  通过对$z_1， z_2$ stop gradient， 让两个网络在训练时候存在些许差异。

SimSiam的结构非常简单：

<img src="https://pic1.zhimg.com/80/v2-8241875b3fca335a9fc431186c401e78_720w.jpg" title="" alt="" data-align="center">

最终SimSiam的效果超过了众多前辈，但仍比BYOL差3个点：

![](https://pic1.zhimg.com/80/v2-5a50b5a3a123f31ca199baec4fa4b1c0_720w.jpg)

同时他们提到，孪生网络自带建模不变性（invariance）的归纳偏置（inductive bias）：

> two observations of the same concept should produce the same outputs  

这个发现可以帮我们理解为什么孪生网络效果很好，表示学习就是要建模数据中的不变性。

---

### 基于蒸馏

#### S2-BNN: Bridging the Gap Between Self-Supervised Real and 1-bit Neural Networks via Guided Distribution Calibration

参考 | https://zhuanlan.zhihu.com/p/393008809

单位 | 1Carnegie Mellon University 2Hong Kong University of Science and Technology Inception Institute of Artificial Intelligence

作者 | [Zhiqiang Shen](https://arxiv.org/search/cs?searchtype=author&query=Shen%2C+Z), [Zechun Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z), [Jie Qin](https://arxiv.org/search/cs?searchtype=author&query=Qin%2C+J), [Lei Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+L), [Kwang-Ting Cheng](https://arxiv.org/search/cs?searchtype=author&query=Cheng%2C+K), [Marios Savvides](https://arxiv.org/search/cs?searchtype=author&query=Savvides%2C+M)

论文 | https://arxiv.org/abs/2102.08946

代码 | https://github.com/szq0214/S2-BNN

##### 结论

本文发现基于小网络的前提下，基于蒸馏(distillation learning)的自监督学习得到的模型性能远远强于对比学习(contrastive learning)，同时他们还发现同时使用蒸馏和对比学习效果反而不如单独使用蒸馏损失，这也是一个非常有意思的发现。

---

#### Seed: Self-supervised distillation for visual representation

单位 | †Arizona State University, ‡Microsoft Corporation

作者 | [Zhiyuan Fang](https://arxiv.org/search/cs?searchtype=author&query=Fang%2C+Z), [Jianfeng Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+J), [Lijuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+L), [Lei Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+L), [Yezhou Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Y), [Zicheng Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+Z)

论文 | https://arxiv.org/pdf/2101.04731

收录 | ICLR2021

##### 出发点

**小模型(即轻量型模型)的自监督学习问题**，作者通过实证发现：**对比自监督学习方法在大模型训练方面表现出了很大进展，然这些方法在小模型上的表现并不好**。

##### 贡献

为解决上述问题，本文提出了一种新的学习框架：自监督蒸馏(Self-SupErvised Distillation, SEED)，它**通过自监督方式(SSL)将老师模型的知识表达能力迁移给学生模型**。不同于直接在无监督数据上的直接学习，我们训练学生模型去模拟老师模型在一组示例上的相似度得分分布。

所提SEED的简洁性与灵活性不言而喻，包含这样三点：(1) 无需任何聚类/元计算步骤生成伪标签/隐类；(2) 老师模型可以通过优秀的自监督学习（比如MoCo-V2、SimCLR、SWAV等）方法进行预训练；(3)老师模型的知识表达能力可以蒸馏到任意小模型中(比如更浅、更细，甚至可以是完全不同的架构)。

##### 模型

![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X2pwZy9WdmtoZFZWVklEZ3NPd25MRE40VmJUMnBzcUh4N1pmMnhOUGRxU09iVFhOOTRxSkVrcmYxaWFjVWVwV2tHVmNXeTFpY2RNYXB4TG90MFd2aWE3MDAyWWliTWcvNjQw?x-oss-process=image/format,png)

##### 结论

实验表明：SEED可以提升小模型在下游任务上的性能表现。相比自监督基准MoCo-V2方案，在ImageNet数据集上，SEED可以将EfficientNet-B0的精度从42.2%提升到67.6%，将MobileNetV3-Large的精度从36.3%提升到68.2%，见下图对比。

![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X2pwZy9WdmtoZFZWVklEZ3NPd25MRE40VmJUMnBzcUh4N1pmMnNGalNxSzdyTm1QaWMwNXhDME9hRjBXTkN4R1N1WkFFUWljRmdicUtaTVQ2UmNqOUlpYWw1VkN6dy82NDA?x-oss-process=image/format,png)

---

#### DINO：Emerging properties in self-supervised vision transformers

参考 | https://zhuanlan.zhihu.com/p/371735639

单位 | 1 Facebook AI Research 2 Inria∗ 3 Sorbonne University

作者 | Mathilde Caron1,2 Hugo Touvron1,3 Ishan Misra1 Herv ́e Jegou1  Julien Mairal2 Piotr Bojanowski1 Armand Joulin1

论文 | [[2104.14294] Emerging Properties in Self-Supervised Vision Transformers](https://arxiv.org/abs/2104.14294)

引用 | ICCV2021

代码 | https://github.com/facebookresearch/dino

##### 出发点

ViT+自监督

##### 模型

![](https://pic1.zhimg.com/80/v2-095bc916db0244a39077438ab6fd70e4_720w.jpg)

![](https://pic2.zhimg.com/80/v2-f3300391fcbf7e6e7d66e13b02922a71_720w.jpg)

##### 实验结果

![](https://pic1.zhimg.com/80/v2-fc66c952290c663e408cac26f0846da8_720w.jpg)

---
