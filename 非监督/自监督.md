#### 自监督分类：生成式、判别式

Reference: https://www.zhihu.com/question/374171361/answer/2289248630

- 生成式：期望利用数据表示重构完整数据

- 判别式：期望数据表示包含足够多信息

![](assets/2022-08-01-09-46-04-2022-08-01%2009-45-45%20的屏幕截图.png)

#### 自监督评价方法

1. 线性评估：对于下游任务，直接在训练好的encoder后面加一个线性分类器，之后测试模型在下游任务上的表现，因为分类器十分简单，所以整体模型的好坏，主要归因于encoder的性能。

2. 下游任务fine tune：设计一个分类器，之后利用下游任务的有标注数据集对数据进行fine tune。将微调后的模型作为执行下游任务的模型。

#### 自监督方式

1. 基于上下文（Context Based）

2. 基于时序（Temporal Based）

3. 基于对比学习（Contrastive Based）

4. 基于蒸馏（distillation loss）

---

#### 3D

---

---

##### 基于上下文：通常人们可以通过理解一整幅图像的语义，并联系图像中图案的结构来补全一个诠释的图像。

###### 1. MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation

参考 | [https://zhuanlan.zhihu.com/p/360486586](https://zhuanlan.zhihu.com/p/360486586)

论文 | https://arxiv.org/pdf/2103.12605

引用 | CVPR2021

---

##### 基于时序

    参考：[https://flashgene.com/archives/106955.html](https://flashgene.com/archives/106955.html)

1. 基于帧之间的相似性：相邻帧特征是相似的，而相隔较远的视频帧是不相似的，通过构建这种相似（position）和不相似（negative）的样本来进行自监督约束。

![](assets/2022-08-01-10-21-04-2022-08-01%2010-20-51%20的屏幕截图.png)

2. 在大量无标注视频中进行无监督追踪，获取大量的物体追中框。同一个物体在不同帧之间的相似， 不同物体的追踪框不同。

![](assets/2022-08-01-10-20-13-2022-08-01%2010-20-03%20的屏幕截图.png)

3. 视频的先后帧顺序：设计一个模型，来判断当前的视频序列是否是正确的顺序。
   
   ![](assets/2022-08-01-10-24-11-2022-08-01%2010-24-01%20的屏幕截图.png)

---

##### 基于对比

###### 1. SimIPU: Simple 2D Image and 3D Point Cloud Unsupervised Pre-training for Spatial-Aware Visual Representations

论文 | https://ojs.aaai.org/index.php/AAAI/article/view/20040/19799

输入：2D图像和Lidar数据

![](assets/2022-08-01-14-37-09-2022-08-01%2014-36-54%20的屏幕截图.png)

点云$p^\alpha$， 经过T变换之后为 $P^\beta$ ，对$P^\alpha 和 p^\beta$ 提取特征，通过 二分图匹配， 计算对比损失；然后 2D image提取特征， feature map， 与$P^\alpha$ 做对比损失。

结果

‘K’ and ‘IN’ indicates pre-trained models are trained on KITTI and ImageNet datset. Best is in bold.

‘W’ indicates the  pre-trained models are trained on Waymo datset.

![](assets/2022-08-01-14-39-43-2022-08-01%2014-39-35%20的屏幕截图.png)

![](assets/2022-08-01-14-41-12-2022-08-01%2014-39-47%20的屏幕截图.png)

点云数据对无监督的3D目标检测， 相较于Mocov2有一定提升。

---

#### 2D

参考：

- [综述：自监督学习与知识蒸馏的碰撞 - 智源社区](https://hub.baai.ac.cn/view/4380)

- https://www.zhihu.com/question/374171361/answer/2289248630

- https://zhuanlan.zhihu.com/p/334732028

- https://zhuanlan.zhihu.com/p/393008809

##### 2D的前置任务（pretext）——提取数据分布潜在规律的关键

| 变形      | 如果能够赋予神经网络这种利用不变性的能力，则可能有效应对对抗性样本的攻击。例如，Dosovitskiy 在于2015年发表的「Exemplar-CNN」中                                                                                         | ![](assets/2022-08-01-18-16-20-2022-08-01%2018-16-09%20的屏幕截图.png) |
| ------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------- |
| 相对位置预测  | 对于每一张输入图像，我们期望网络可以学习到不同图块之间的相对位置关系。Doersch 等人于 2015 年发表的论文「Unsupervised Visual Representation Learning by Context Prediction」将 pretext 任务形式化定义为预测同一张图像中随机两个图块之间的相对位置。 | ![](assets/2022-08-01-18-18-18-2022-08-01%2018-18-11%20的屏幕截图.png) |
| 图像着色    | 图像自动上色也可以作为一种有效的前置任务。在 ECCV 2016 上发表的论文「Colorful Image Colorization」中，作者提出通过量化色彩空间，将着色问题转化而我分类问题，将该图像映射到量化的色彩值输出的分布上。                                                 | ![](assets/2022-08-01-18-18-34-2022-08-01%2018-18-25%20的屏幕截图.png) |
| 生成式图像恢复 | 自编码器是一类在自监督学习中被广泛使用的网络架构，旨在通过重建被扰动的图像提取出关于图像的本质特征。在论文「Context Encoders: Feature Learning by Inpainting」中，作者训练了一个上下文编码器来填补图像中缺失的一块，并使用了一个 L2 重建损失和对抗性损失的组合来训练该模型。      | ![](assets/2022-08-01-18-18-45-2022-08-01%2018-18-36%20的屏幕截图.png) |

##### 基于对比

###### Unsupervised Object-Level Representation Learning from Scene Images

单位 | 

作者 | 

论文 | NeurIPS 2021 https://arxiv.org/abs/2106.11952

引用 |

---

###### MoCo: Momentum Contrast for Unsupervised Visual Representation Learning

单位 |Facebook AI Research (FAIR)

作者 |Kaiming He Haoqi Fan Yuxin Wu Saining Xie Ross Girshick

论文 | https://arxiv.org/pdf/1911.05722

引用 |CVPR2020

代码 | https://github.com/facebookresearch/moco

主要思想： 

- Memory Bank容量大， 导致了采样的特征具有不一致性，因为 训练一段时间后， 更新了参数， 导致 encoder结果在 特征空间 不一致

- MoCo采用队列存储和采样Negetive 样本， 队列中存储多个近期用于训练的batch的特征向量。队列存储的是图像特征， 因为队列的FIFO特性， 保证 很长时间前的数据 pop，不会被取到。

<img src="file:///home/yihang/.config/marktext/images/2022-07-27-15-32-02-2022-07-27%2015-31-40%20的屏幕截图.png" title="" alt="" data-align="center">

**对于每个batch x：**

1. 随机增强出 $x^q、x^k$两种view
2. 分别用$f_q、f_k$  对输入进行编码得到归一化的 q 和 k，并去掉 k 的梯度更新
3. 将 q 和 k 一一对应相乘得到正例的cosine（Nx1），再将 q 和队列中存储的K个负样本相乘（NxK），拼接起来的到 Nx(1+K) 大小的矩阵，这时第一个元素就是正例，直接计算交叉熵损失，更新$f_q$的参数
4. 动量更新$f_k$的参数：$f_k=m*f_k+(1-m)*f_q$
5. 将 k 加入队列，把队首的旧编码出队，负例最多时有65536个

**实验结果**

![](https://pic2.zhimg.com/80/v2-572c54f3c37309e9f4005eab94aaf08d_720w.jpg)

---

###### SimCLR: A Simple Framework for Contrastive Learning of Visual Representations

作者 |Ting Chen 1 Simon Kornblith 1 Mohammad Norouzi 1 Geoffrey Hinton

论文 | https://arxiv.org/pdf/2002.05709

引用 |ICML2020

        SimCLR是Hinton组的Chen Ting在20年2月提出的工作，直接比MoCo高出了7个点，并直逼监督模型的结果。

<img src="https://pic4.zhimg.com/80/v2-dc3f182be959bfd5757d5f6ffe98410b_720w.jpg" title="" alt="" data-align="center">

**改动**：

1. 探究了不同的数据增强组合方式，选取了最优的
2. 在encoder之后增加了一个非线性映射 $g(h_i)=W^2 ReLU(W^1 h_i)$。研究发现encoder编码后的$h$会保留和数据增强变换相关的信息，而非线性层的作用就是去掉这些信息，让表示回归数据的本质。注意非线性层只在无监督训练时用，在迁移到其他任务时不使用
3. 计算loss时多加了负例。以前都是拿右侧数据的N-1个作为负例，SimCLR将左侧的N-1个也加入了进来，总计2(N-1)个负例。另外SImCLR不采用memory bank，而是用更大的batch size，最多的时候bsz为8192，有16382个负例

**实验结果**

![](https://pic4.zhimg.com/80/v2-1a8225d520897fbc359f53fe075ebf67_720w.jpg)

---

###### MoCo v2:  Improved Baselines with Momentum Contrastive Learning

单位 |Facebook AI Research (FAIR)

作者 | Xinlei Chen, Haoqi Fan, Ross Girshick, Kaiming He

论文 | https://arxiv.org/pdf/2003.04297

SimCLR推出后一个月，何凯明和Chen Xinlei同学对MoCo进行了一些小改动：

1. 改进了数据增强方法
2. 训练时在encoder的表示上增加了相同的非线性层
3. 为了对比，学习率采用SimCLR的cosine衰减

<img src="https://pic2.zhimg.com/80/v2-66d1f1e5721901c1c3a24cd34f19dfc1_720w.jpg" title="" alt="" data-align="center">

经过改动后，以更小的batch size就超过了SimCLR的表现：

实验结果

![](https://pic2.zhimg.com/80/v2-a7dbc04be15d66cbcb5fbaf6eccb3d69_720w.jpg)

---

###### SimCLR v2：Big Self-Supervised Models are Strong Semi-Supervised Learners

单位 | Google Research, Brain Team

作者 | Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, Geoffrey Hinton

论文 |  https://arxiv.org/abs/2006.10029

引用 |NIPS2020

代码 | https://github.com/google-research/simclr

在2020年中，Hinton组的Chen Ting同学又提出了SimCLR v2**[4]**，主要做了以下改动：

1. 采用更深但维度略小的encoder，从 ResNet-50 (4×) 改到了 ResNet-152 (3×+SK)，在1%的监督数据下提升了29个点
2. 采用更深的3层MLP，并在迁移到下游任务时保留第一层（以前是完全舍弃），在1%的监督数据下提升了14个点
3. 参考了MoCo使用memory，但由于batch已经足够大了，只有1%左右的提升

最终模型比之前的SOTA好了不少：

<img src="https://pic2.zhimg.com/80/v2-07718a4d79ffa88c8b12a10844f12349_720w.jpg" title="" alt="" data-align="center">

---

###### SwAV： Unsupervised Learning of Visual Features by Contrasting Cluster Assignments

单位 | 1 Inria∗ 2 Facebook AI Research

作者 | Mathilde Caron1,2 Ishan Misra2 Julien Mairal1  Priya Goyal2 Piotr Bojanowski2 Armand Joulin2

论文 | https://proceedings.neurips.cc/paper/2020/file/70feb62b69f16e0238f741fab228fec2-Paper.pdf

引用 |NIPS2020

        对比学习需要很多负例进行比较，既耗时又耗内存，于是FAIR联合Inria也推出了一个新的方法SwAV。作者提出了一个新的想法：**对各类样本进行聚类，然后去区分每类的类簇**。模型结构如下：

![](https://pic3.zhimg.com/80/v2-5c57e440e11930c0397ddbdb0164c89e_720w.jpg)

        具体的做法是，先用K个点$C=c_1, c_2...c_k$  表示聚类中心（prototypes），给定一个batch的编码 $Z=z_1, z_2, ..., z_B$，将其通过$C$映射到一组新的向量$Q=q_1, q_2, ..., q_B$。这里假设向量都是d维的，那C的维度就是 dxK，Z的维度是 dxB，Q的维度则是KxB，每个元素$q_{kb}$相当于第k个聚类中心与第b个样本的相似度，理想情况下，样本与自己的类簇中心相似度为1，与其他的为0，其实就是一个one-hot label。不过作者发现soft label效果会好一些。这样每个样本又获得了一个新的表示（Codes）。

        有了z和q之后，理论上同一张图片不同view所产生的z和q也可以相互预测，作者便定义了新的loss：$L(z_t, z_s)=l(z_t, q_s)+l(z_s, q_t)$

        其中$l(z_t, q_s)=-\sum\limits_{k}q_s^k\;log\,p_t^k, p_t=\frac{exp(z_t^Tc_k/\tau)}{\sum\limits_{k^{\prime}}exp(z_t^Tc^{k^{\prime}/\tau})}$

        同时SwAV也提出了一种新的数据增强方法，将不同分辨率的view进行mix。最终两种方法的结合带来了4.2个点的提升:

<img src="https://pic3.zhimg.com/80/v2-a6faf2ec006afe9ce5665bbe4c22ce3a_720w.jpg" title="" alt="" data-align="center">

---

###### SEER： Self-supervised Pretraining of Visual Features in the Wild

单位 | 1 Facebook AI Research 2 Inria*

作者 | [Priya Goyal](https://arxiv.org/search/cs?searchtype=author&query=Goyal%2C+P), [Mathilde Caron](https://arxiv.org/search/cs?searchtype=author&query=Caron%2C+M), [Benjamin Lefaudeux](https://arxiv.org/search/cs?searchtype=author&query=Lefaudeux%2C+B), [Min Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+M), [Pengchao Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+P), [Vivek Pai](https://arxiv.org/search/cs?searchtype=author&query=Pai%2C+V), [Mannat Singh](https://arxiv.org/search/cs?searchtype=author&query=Singh%2C+M), [Vitaliy Liptchinsky](https://arxiv.org/search/cs?searchtype=author&query=Liptchinsky%2C+V), [Ishan Misra](https://arxiv.org/search/cs?searchtype=author&query=Misra%2C+I), [Armand Joulin](https://arxiv.org/search/cs?searchtype=author&query=Joulin%2C+A), [Piotr Bojanowski](https://arxiv.org/search/cs?searchtype=author&query=Bojanowski%2C+P)

论文 | https://arxiv.org/abs/2103.01988

代码 | [Code found at https://github.com/facebookresearch/vissl](https://www.catalyzex.com/redirect?url=https://github.com/facebookresearch/vissl)

        SEER是SwAV的作者们在21年3月发布的新工作，概括来说就是**用更好的模型、更多的数据。**

        首先是数据，之前的模型都是在一百万左右的ImageNet上训练，而这些数据都是挑选过的，只能代表开放域中的一部分数据。于是作者就想：**在更大的开放域数据上训练是否能提升效果呢？**如果可行的话，那以后就在更更多的数据上pretrain，然后再迁移到下游任务就可以了。于是作者收集了10亿的Instagram图片来做训练。

        然后是模型，作者选用了凯明大神去年推出的RegNet，结合了神经网络搜索NAS的优点，在Imagenet上搜索出特定FLOPs下不错的结构。最后在参数比SimCLRv2少的情况下达到了更好的效果：

<img src="https://pic3.zhimg.com/80/v2-06d4d9db821c0afd734a6b2f69391eee_720w.jpg" title="" alt="" data-align="center">

        虽然证明了在开放域数据上的预训练确实有效果，**但少样本的情况下还是不如直接在ImageNet上训。不过迁移能力确实很好，在Places205数据集上评估，比ImageNet有监督预训练的模型好，说明**无监督预训练让模型学到更多通用知识。

---

###### BYOL：Bootstrap Your Own Latent A New Approach to Self-Supervised Learning

参考 | https://zhuanlan.zhihu.com/p/352364087

单位 | 1DeepMind 2Imperial College

作者 | Jean-Bastien Grill∗,1 Florian Strub∗,1 Florent Altché∗,1 Corentin Tallec∗,1 Pierre H. Richemond∗,1,2  Elena Buchatskaya1 Carl Doersch1 Bernardo Avila Pires1 Zhaohan Daniel Guo1  Mohammad Gheshlaghi Azar1 Bilal Piot1 Koray Kavukcuoglu1 Rémi Munos1 Michal Valko1

论文 | https://arxiv.org/abs/2006.07733



代码 | https://github.com/lucidrains/byol-pytorch

        上文讲的方法来回都逃不过“对比”这个范式，而DeepMind提出的BYOL则给了我们一个不同视角。

        在表示学习中，我们现在采用的框架本质是通过一个view的表示去预测相同图像其他view，能预测对说明抓住了图像的本质特征。但在做这样的预测时会有坍缩（collapse）的风险，意思是全都变成一个表示，那也可以做到预测自己。对比学习为了解决这个问题，将表示预测问题转换为了正负例判别问题，这样就迫使模型的输出是多样的，避免坍缩。

        于是BYOL的作者想：如何不用负例，也能学到好的表示呢？如果共用encoder，用MSE作为损失，缩小相同图像不同view的距离，肯定会坍缩。而作者发现如果把其中一个encoder变成随机初始化的固定下来（stop gradient），就能达到18.8%的准确率。为了得到更好的encoder，作者参考动量的方法对其中一个encoder做了改进：

<img src="https://pic3.zhimg.com/80/v2-f4e062422f439c21aad3fc1480598dbe_720w.jpg" title="" alt="" data-align="center">

        上半部分为online（更新梯度），下半部分为target（不更新梯度）。BYOL的优化目的是用online表示预测target表示，采用MSE作为损失函数。Online梯度回传后，使用滑动平均对targe的encoder和MLP参数进行更新。在预测阶段只使用$f_\theta$。

        虽然BYOL没有显示地使用对比学习loss，但一篇博主在实验中发现BYOL依靠的还是“对比”。他们在复现BYOL的时候直接基于了MoCo的代码，结果发现效果还没有随机的好，原来是因为MLP中没有加BN。如果深究BN的作用，就会发现它重新调整了输出的分布，避免坍缩，同时**BN也在隐式地进行对比，去除batch内样本相同的部分，保留不同的特征**。

同时，在不依赖负样本后，BYOL对于数据增强方法的选择更加鲁棒，下面是它的效果：

<img src="https://pic4.zhimg.com/80/v2-5259ea1d1af06d65b56e6798496b55a7_720w.jpg" title="" alt="" data-align="center">

---

---

###### SimSiam： Exploring Simple Siamese Representation Learning

单位 | Facebook AI Research (FAIR)

作者 | Xinlei Chen Kaiming He

论文 | https://arxiv.org/abs/2011.10566

引用 | CVPR2021

代码 | https://github.com/facebookresearch/simsiam

        延续BYOL的思想，Chen Xinlei与何凯明大佬又对孪生网络进行了研究，发现stop-gradient是避免坍缩的关键，于是提出了SimSiam。

SimSiam的结构非常简单：

<img src="https://pic1.zhimg.com/80/v2-8241875b3fca335a9fc431186c401e78_720w.jpg" title="" alt="" data-align="center">

最终SimSiam的效果超过了众多前辈，但仍比BYOL差3个点：

![](https://pic1.zhimg.com/80/v2-5a50b5a3a123f31ca199baec4fa4b1c0_720w.jpg)

同时他们提到，孪生网络自带建模不变性（invariance）的归纳偏置（inductive bias）：

> two observations of the same concept should produce the same outputs  

这个发现可以帮我们理解为什么孪生网络效果很好，表示学习就是要建模数据中的不变性。

---

##### 基于蒸馏

###### S2-BNN: Bridging the Gap Between Self-Supervised Real and 1-bit Neural Networks via Guided Distribution Calibration

参考 | https://zhuanlan.zhihu.com/p/393008809

作者 | Zhiqiang Shen1, Zechun Liu1,2, Jie Qin3, Lei Huang3, Kwang-Ting Cheng2, Marios Savvides1

单位 | 1Carnegie Mellon University 2Hong Kong University of Science and Technology Inception Institute of Artificial Intelligence

论文 | https://arxiv.org/abs/2102.08946

引用 | https://github.com/szq0214/S2-BNN

---

Seed: Self-supervised distillation for visual representation.

---

DINO：Emerging properties in self-supervised vision transformers.

---
