#### 自监督数学理论分析

##### 1. 为什么 stop gradient能有效？

参考 | https://zhuanlan.zhihu.com/p/372521032

作者 | 田渊栋Meta FAIR研究院研究员/高级经理，CMU机器人系博士

论文 | [Understanding self-supervised Learning Dynamics without Contrastive Pairs](https://arxiv.org/pdf/2102.06810)

收录 | ICML2021

###### 出发点

这篇文章就是针对最近出现的一些新型的，无负例样本对的表示学习算法（如BYOL和SimSiam），进行了一些梯度下降动力学上的分析。

###### 实验分析

![](https://pic1.zhimg.com/80/v2-bda2a3751fd4dc1b20ad1f096f7daf98_720w.jpg)

![](https://pic2.zhimg.com/80/v2-816b8f937da896b8cd24ba8edd595f2d_720w.jpg)

![](https://pic2.zhimg.com/80/v2-e32b6cb32c42db9ecbfb552b97fb6e1d_720w.jpg)

发现了在训练过程中， 预测器$W_p$和预测器输入的相关矩阵 $F=W\mathbb{E} [xx^T]W^T$这两者的特征空间会逐渐地趋向对齐（eigenspace alignment），这也被实验所充分证明:

![](https://pic2.zhimg.com/80/v2-6a1d5a982ecedc180726b64c97ada759_720w.jpg)

![](https://pic4.zhimg.com/80/v2-6fd0fd8c5b784c0fa3fa107ba8c04e6b_720w.jpg)

**前景知识**：<mark>实对称矩阵可交换的前提， 是相似于同一个 实对角矩阵 </mark>

![](https://pic4.zhimg.com/80/v2-a434b0a586ee37c993bc00dfddf98827_720w.jpg)

针对这个一维问题我们可以进行大量的细致分析，比如说我们可以发现$W_p$的特征值有好几个稳定解。其中零点是平凡解，但还有一个非零的非平凡解，BYOL类的算法在训练时为什么能不崩溃，为什么可以收敛到好的表示，都是因为有这个非平凡解的存在。

![](assets/2022-08-05-14-26-04-2022-08-05%2014-25-49%20的屏幕截图.png)
